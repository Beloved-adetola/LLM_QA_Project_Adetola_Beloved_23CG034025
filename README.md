# ğŸ§  LLM Question-and-Answering System  
A simple NLP-powered Question & Answering system featuring:

- A **Python CLI application**
- A **Web GUI** (Flask-based)
- Integration with the **Hugging Face Inference API (Chat Completions)**
- Easy deployment on Render.com or other hosting platforms

---

## ğŸš€ Features
- Accept natural-language questions from users  
- Preprocess input (lowercasing, cleaning, math-friendly tokenization)  
- Forward processed question to an LLM through Hugging Faceâ€™s API  
- Display processed question, raw API output, and final answer  
- Clean modern web interface  
- Simple CLI interface  

---

## ğŸ“‚ Project Structure

/LLM_QA_Project_YourName_MatricNo/
â”‚â”€â”€ LLM_QA_CLI.py
â”‚â”€â”€ app.py
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ LLM_QA_hosted_webGUI_link.txt
â”‚â”€â”€ .env
â”‚â”€â”€ /static/
â”‚ â””â”€â”€ style.css
â”‚â”€â”€ /templates/
â””â”€â”€ index.html

---

## ğŸ”§ Installation & Setup

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/LLM_QA_Project.git
cd LLM_QA_Project


ğŸ” 2. Set Up Your Hugging Face API Key

Create a file named .env in the project root:

HUGGINGFACE_API_KEY=hf_xxxxxxxxxxxxxxxxx


The app automatically loads environment variables using python-dotenv.



ğŸ“¦ 3. Install Dependencies

Run:

pip install -r requirements.txt


Typical requirements.txt:

Flask
requests
python-dotenv
gunicorn



ğŸ–¥ï¸ Running the CLI Application

Run:

python LLM_QA_CLI.py


Then type your question:

> Enter your question: what is 2+2?


The CLI will:

preprocess the question

send it to Hugging Face

print the final answer



ğŸŒ Running the Web GUI (Flask App)

Start the Flask server:

python app.py


Then open this URL in your browser:

http://127.0.0.1:5000


Youâ€™ll be able to:

Enter a question

View the processed version

View the raw LLM response

See the final generated answer